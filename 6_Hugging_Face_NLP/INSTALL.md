### Virtual Environment  (Python 3.10.9)
    python3 -m venv myenv
    source myenv/bin/activate
    pip install transformers
    pip install tensorflow-macos
    pip install tf-keras
    pip install torch
    deactivate (optional)

### Run

### Test

### requirements.txt
    pip freeze > requirements.txt



## Large language models
Large language models are pre-trained models that can understand and generate human-like text. We can refer to these language models as LLM as well. As these models are built on vast amounts of data with a greater number of parameters, we call them LLMs.
Use cases include human-like text generation, translation, text summarization, a questions-answering system, and more.
Some of such models are GPT-3, GPT-4, Gemini, LLaMA, T5 etc.

Large Language Models (LLMs) are considered to be a core component of Natural Language Processing (NLP) and Natural Language Generation (NLG).

BERT and pre-trained language models (2018-2019):  In 2018, Google's revolutionary Bidirectional Encoder Representations from Transformers (BERT)

## LLM use cases
Text generation:
Translation:
Text summarization:
Question answering:
Sentiment analysis:
Code generation:

## Transformers
Within the context of large language models, a transformer is the underlying architecture or framework that enables the model to process and understand language. The transformer lets the model analyze relationships between phrases, take into account the context of a sentence and generate coherent and contextually relevant text. In essence, it ºs the technological spine that empowers LLMs to perform advanced tasks like answering questions, completing sentences, or maybe producing innovative textual content based totally on the styles it has learned all through training.
In most cases, transformers have replaced CNN and RNN networks. 